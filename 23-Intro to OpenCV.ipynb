{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image manipulation\n",
    "\n",
    "* A lot of our data comes from images\n",
    "  * We want to manipulate them!\n",
    "* We know that images can be represented as pixels in a matrix\n",
    "  * We can now manipulate them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start by getting a picture. Since parts of this lecture are based on Pyimagesearch's basic image manipulation tutorial, see http://www.pyimagesearch.com/2014/01/20/basic-image-manipulations-in-python-and-opencv-resizing-scaling-rotating-and-cropping/, we make use of the same image:\n",
    "\n",
    "![](http://www.pyimagesearch.com/wp-content/uploads/2014/01/jurassic-park-tour-jeep.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Later in the tutorial, we will use another image:\n",
    "\n",
    "![](http://1.bp.blogspot.com/-lzNnsDrxp-A/U7UEUDx9-aI/AAAAAAABDEM/_lWD43uxKOI/s1600/mare-08.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://www.pyimagesearch.com/wp-content/uploads/2014/01/jurassic-park-tour-jeep.jpg\n",
    "wget http://1.bp.blogspot.com/-lzNnsDrxp-A/U7UEUDx9-aI/AAAAAAABDEM/_lWD43uxKOI/s1600/mare-08.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intro to OpenCV \n",
    "\n",
    "*OpenCV* (Open Source Computer Vision Library) is an open source computer vision and machine learning software library with more than 2500 optimized algorithms. The library is written in optimized C/C++ and has interfaces for various languages.\n",
    "\n",
    "\n",
    "```bash\n",
    "conda install -c menpo opencv\n",
    "```\n",
    "\n",
    "https://opencv.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reading an Image from Disk\n",
    "You can read an image as in the following. Note, that the swap of color channels (`cv2.COLOR_BGR2RGB`) is only necessary for inlining a picture with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# https://github.com/mciantyre/scipy-opencv-notebook\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image = cv2.imread('./jurassic-park-tour-jeep.jpg')\n",
    "# the swap of color channels is only necessary for inlining a picture with matplotlib\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.title('Jurrasic Park')\n",
    "plt.imshow(image, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you do not want to plot a picture inlined but open an external image viewer, you can use the `cv2.imshow(\"Image Name\", img)` in stand-alone programs, i.e., not in Jupyter Notebooks.\n",
    "\n",
    "\n",
    "## Resizing an Image\n",
    "\n",
    "You can use various interpolation mechanisms when resizing. OpenCV is really well documented and you can find more on `cv2.resize`'s interpolation options here: http://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=resize#resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def read(path, switch_channels=True):\n",
    "    image = cv2.imread(path)\n",
    "    if switch_channels:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def resize(image, new_x_dim):\n",
    "    ratio = new_x_dim / image.shape[1]\n",
    "    dim = (new_x_dim, int(image.shape[0] * ratio))\n",
    "    \n",
    "    # perform the actual resizing of the image and show it\n",
    "    resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def create_plot(image, title=''):\n",
    "    plt.imshow(image, interpolation='none')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# image_path = './jurassic-park-tour-jeep.jpg'\n",
    "image_path = './mare-08.jpg'\n",
    "\n",
    "img = read(image_path)\n",
    "img_small = resize(img, 320)\n",
    "create_plot(img_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Detecting Objects in an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://www.sportsnewsempire.com/wp-content/uploads/2017/03/verdasco-dubai-2017-thursday.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_path = './verdasco-dubai-2017-thursday.jpg'\n",
    "\n",
    "img = read(image_path)\n",
    "create_plot(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Color Spaces\n",
    "\n",
    "We have seen the RGB color space so far, where three channels contain the information about how much red, green, and blue contribute to the color of a pixel.\n",
    "\n",
    "A disadvantage with the RGB color space is, that pixel values change quite much for the same color under different light conditions.\n",
    "\n",
    "Alternatives to the RGB color space are for example the HSV (Hue, Saturation, Value) color space and the Lab color space, where *L* stands for lightness and *a* and *b* for the color opponents green–red and blue–yellow.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/HSV_color_solid_cone_chroma_gray.png/640px-HSV_color_solid_cone_chroma_gray.png)\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/CIELAB_color_space_top_view.png/497px-CIELAB_color_space_top_view.png)\n",
    "\n",
    "\n",
    "Using the HSV color space, we can find for example the tennis ball quite easily. We create a mask for a certain range of color values, which are characterisitc for a tennis ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def create_ball_mask(image):\n",
    "\n",
    "    # Color values in HSV\n",
    "    green_lower = (20, 100, 180)\n",
    "    green_upper = (60, 255, 255)\n",
    "\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    " \n",
    "    mask = cv2.inRange(hsv, green_lower, green_upper)\n",
    "    mask = cv2.dilate(mask, None, iterations=2)\n",
    "    mask = cv2.erode(mask, None, iterations=2)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "create_plot(create_ball_mask(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mark_object(image, mask):\n",
    "\n",
    "    contours = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "\n",
    "    # find the largest contour in the mask, then use\n",
    "    # it to compute the minimum enclosing circle and\n",
    "    # centroid\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "\n",
    "    # draw the circle and centroid on the frame,\n",
    "    # then update the list of tracked points\n",
    "    cv2.circle(image, (int(x), int(y)), int(radius), (255, 0, 0), 5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mask = create_ball_mask(img)\n",
    "img = mark_object(img, mask)\n",
    "cv2.imwrite('./verdasco-obj-detected.jpg', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "![](./verdasco-obj-detected.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://media.npr.org/assets/img/2016/12/20/ap_16281584372854-061869e770af197e85badc7147ade2f8a0ac582c-s900-c85.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "![](ap_16281584372854-061869e770af197e85badc7147ade2f8a0ac582c-s900-c85.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_path = 'ap_16281584372854-061869e770af197e85badc7147ade2f8a0ac582c-s900-c85.jpg'\n",
    "\n",
    "img = read(image_path)\n",
    "mask = create_ball_mask(img)\n",
    "img = mark_object(img, mask)\n",
    "\n",
    "cv2.imwrite('./obj-detected.jpg', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "create_plot(create_ball_mask(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "![](./obj-detected.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "![](verdasco-dubai-2017-thursday.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_path = './verdasco-dubai-2017-thursday.jpg'\n",
    "\n",
    "img = read(image_path)\n",
    "\n",
    "green_lower = (200, 180, 70)\n",
    "green_upper = (255, 255, 175)\n",
    " \n",
    "# construct a mask for the color \"green\", then perform\n",
    "# a series of dilations and erosions to remove any small\n",
    "# blobs left in the mask\n",
    "mask = cv2.inRange(img, green_lower, green_upper)\n",
    "\n",
    "print(mask.any(), mask.all())\n",
    "create_plot(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mask = cv2.erode(mask, None, iterations=2)\n",
    "mask = cv2.dilate(mask, None, iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reducing the Amount of Colors in an Image\n",
    "\n",
    "\n",
    "\n",
    "The following code is taken from the example on http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py\n",
    "\n",
    "The process of reducing the number of distinct colors in an image is often called color quantization. The idea is to preserve the color appearance of an image and to reduce the number of colors, e.g. for compression.\n",
    "\n",
    "You can make use of the Mean Shift algorithm from two lectures ago to perform color quantization. This will find you all of the 'most prominent' colors. However, the drawback is that the Mean Shift algorithm has exponential runtime, which makes it unsuitable for color quantization in bigger images.\n",
    "\n",
    "There is a nice tutorial on http://www.pyimagesearch.com/2014/07/07/color-quantization-opencv-using-k-means-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "\n",
    "\n",
    "def apply_kmeans(image, n_colors=16):\n",
    "    # Convert to floats instead of the default 8 bits integer coding. Dividing by\n",
    "    # 255 is important so that plt.imshow behaves works well on float data (need to\n",
    "    # be in the range [0-1])\n",
    "    image = np.array(image, dtype=np.float64) / 255\n",
    "\n",
    "    # Load Image and transform to a 2D numpy array.\n",
    "    w, h, d = image.shape\n",
    "    assert d == 3\n",
    "    image_array = np.reshape(image, (w * h, d))\n",
    "\n",
    "    print(\"Fitting model on a small sub-sample of the data\")\n",
    "    t0 = time()\n",
    "    image_array_sample = shuffle(image_array, random_state=0)[:1000]\n",
    "    kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Get labels for all points\n",
    "    print(\"Predicting color indices on the full image (k-means)\")\n",
    "    t0 = time()\n",
    "    labels = kmeans.predict(image_array)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    return labels, kmeans.cluster_centers_, w, h\n",
    "\n",
    "\n",
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d))\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i][j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "\n",
    "n_colors = 16\n",
    "\n",
    "image_path = 'mare-08.jpg'\n",
    "image = read(image_path)  # load_sample_image(\"china.jpg\")\n",
    "labels, cluster_centers, w, h = apply_kmeans(image, n_colors=n_colors)\n",
    "quant_image = recreate_image(cluster_centers, labels, w, h)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "plt.imshow(image, interpolation='none')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Quantized image ({} colors, K-Means)'.format(n_colors))\n",
    "plt.axis('off')\n",
    "plt.imshow(quant_image, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "![](mare-08.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Color Quantization and Color Replacement\n",
    "\n",
    "Let's do a bit of art. Say we have a modern photo and we want to make it appear as back in the days on a Commodore 64.\n",
    "Similar to this: http://realstuffforabstractpeople.com/retrofyme/\n",
    "\n",
    "\n",
    "![](http://www.nightfallcrew.com/wp-content/gallery/industrial_dawn_demo/industrial_dawn_demo_0.png)\n",
    "\n",
    "Consequently, since the C64 can only display 16 colors, we could write a small program, which applies color quantization as above and subsequently, we find the nearest color in the C64 color palette, see https://en.wikipedia.org/wiki/List_of_color_palettes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def _c64_colors():\n",
    "    # From https://upload.wikimedia.org/wikipedia/commons/6/65/Commodore64_palette.png\n",
    "    # with Seashore, The 16 C64 colors\n",
    "    black = [0, 0, 0]\n",
    "    white = [255, 255, 255]\n",
    "    red = [154, 76, 67]\n",
    "    cyan = [122, 194, 200]\n",
    "    purple = [157, 90, 165]\n",
    "    green = [103, 171, 95]\n",
    "    blue = [82, 73, 156]\n",
    "    yellow = [202, 212, 137]\n",
    "    a = [156, 103, 58]\n",
    "    b = [106, 82, 12]\n",
    "    c = [197, 126, 119]\n",
    "    d = [99, 99, 99]\n",
    "    e = [139, 139, 139]\n",
    "    f = [164, 226, 157]\n",
    "    g = [139, 130, 205]\n",
    "    hc = [175, 175, 175]\n",
    "\n",
    "    c64_colors = [black, white, red, cyan, purple, green, blue, yellow, \n",
    "                  a, b, c, d, e, f, g, hc]\n",
    "    return c64_colors\n",
    "\n",
    "\n",
    "def _get_closest_c64_color(value):\n",
    "    dst = 200000\n",
    "    for color in _c64_colors():\n",
    "        new_dst = distance.euclidean(color, value)\n",
    "        if new_dst < dst:\n",
    "            dst = new_dst\n",
    "            return_color = color\n",
    "    return return_color\n",
    "\n",
    "\n",
    "def to_c64_colors(image):\n",
    "    \"\"\"\n",
    "    image:\n",
    "        A 3d numpy array\n",
    "    \"\"\"\n",
    "    c64_img = np.copy(image)\n",
    "    h, w, _ = c64_img.shape\n",
    "    \n",
    "    for x in tqdm(range(w)):\n",
    "        for y in range(h):\n",
    "            c64_img[y, x] = _get_closest_c64_color(c64_img[y, x])\n",
    "    # [[_get_closest_c64_color(c64_img[y, x]) for y in range(h)] for x in range(w)]\n",
    "    return c64_img\n",
    "\n",
    "\n",
    "small_image = resize(image, 320)\n",
    "c64_image = to_c64_colors(small_image)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original'.format(n_colors))\n",
    "plt.axis('off')\n",
    "plt.imshow(image, interpolation='none')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('What it could be on a C64'.format(n_colors))\n",
    "plt.axis('off')\n",
    "plt.imshow(c64_image, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Finding Nearest-neighbours with KD-Trees\n",
    "\n",
    "As you can see, the naive implementation above is quite slow. We could speed it up, by first apply a color \n",
    "\n",
    "However, we can also make use of a KD-Tree containing the \"nearest\" colors.\n",
    "\n",
    "See:\n",
    "  * https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html\n",
    "  * https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html\n",
    "  * https://en.wikipedia.org/wiki/K-d_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.spatial as sp\n",
    "\n",
    "\n",
    "\n",
    "def create_tree(colors):\n",
    "    tree = sp.cKDTree(colors)  # creating kd-tree from C64 colors\n",
    "    return tree\n",
    "\n",
    "\n",
    "def query_tree(small_image, tree):\n",
    "    h, w, d = small_image.shape\n",
    "    small_image_lst = small_image.reshape(h * w, d)\n",
    "    distance, result = tree.query(small_image_lst)  # get Euclidean distance and index of each C64 color in tree\n",
    "\n",
    "    for idx, c in enumerate(_c64_colors()):\n",
    "        small_image_lst[result == idx] = c\n",
    "    return small_image_lst.reshape(h, w, d)\n",
    "\n",
    "\n",
    "tree = create_tree(_c64_colors())\n",
    "small_image_lst = query_tree(small_image, tree)      \n",
    "\n",
    "plt.title('What it could be on a C64 with {} colors'.format(n_colors))\n",
    "plt.axis('off')\n",
    "plt.imshow(small_image_lst, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "small_image_lst = query_tree(small_image, tree)      "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
