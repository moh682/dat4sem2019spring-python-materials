{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Analytics\n",
    "\n",
    "Watson Analytics is a Cloud Service offering AI capabilities via REST APIs. https://www.ibm.com/analytics/watson-analytics/us-en/index.html\n",
    "\n",
    "\n",
    "https://github.com/watson-developer-cloud/python-sdk\n",
    "\n",
    "```bash\n",
    "pip install watson-developer-cloud\n",
    "```\n",
    "The API documentation is here:\n",
    "https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/curl.html?curl\n",
    "\n",
    "\n",
    "## Getting the Service Credentials\n",
    "\n",
    "Service credentials are required to access the APIs.\n",
    "\n",
    "To run locally or outside of Bluemix you need the `username` and `password` credentials for each service. (Service credentials are different from your Bluemix account email and password.)\n",
    "\n",
    "To create an instance of the service:\n",
    "\n",
    "  * Log in to [Bluemix](https://console.ng.bluemix.net).\n",
    "  * Create an instance of the service:\n",
    "    1. In the Bluemix Catalog, select the Watson service you want to use. For our example, select under *Watson* the *Visual Recognition* service.\n",
    "    2. Type a unique name for the service instance in the Service name field. For example, type my-service-name. Leave the default values for the other options.\n",
    "    3. Click Create.\n",
    "    \n",
    "    \n",
    "To get your service credentials:\n",
    "\n",
    "  * Copy your credentials from the *Service Details* page. To find the the *Service Details* page for an existing service, navigate to your Bluemix dashboard and click the service name.\n",
    "  * On the *Service Details* page, click *Service Credentials*, and then *View Credentials*.\n",
    "Copy username and password.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"IBM_WATSON_API_KEY='YOUR_API_KEY'\" >> ./api_keys.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import api_keys\n",
    "\n",
    "\n",
    "API_KEY = {\n",
    "  \"url\": \"https://gateway-a.watsonplatform.net/visual-recognition/api\",\n",
    "  \"note\": \"It may take up to 5 minutes for this key to become active\",\n",
    "  \"api_key\": api_keys.IBM_WATSON_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Network with Pre-trained Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example:\n",
    "# https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py\n",
    "import json\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "\n",
    "\n",
    "test_url = 'https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg'\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-05-20', api_key=API_KEY['api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition.classify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the Watson's Visual Recognition Classifier says to IBM's CEO Ginni Rometty.\n",
    "\n",
    "![](https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"womans portrait photo\",\n",
      "              \"score\": 0.599,\n",
      "              \"type_hierarchy\": \"/person/female/woman/womans portrait photo\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"woman\",\n",
      "              \"score\": 0.601\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"female\",\n",
      "              \"score\": 0.602\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.725\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"adult person\",\n",
      "              \"score\": 0.559,\n",
      "              \"type_hierarchy\": \"/person/adult person\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/people\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"light brown color\",\n",
      "              \"score\": 0.718\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"beige color\",\n",
      "              \"score\": 0.533\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\",\n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.classify(url=test_url), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition.detect_faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 48,\n",
      "            \"max\": 51,\n",
      "            \"score\": 0.76694834\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 245,\n",
      "            \"width\": 237,\n",
      "            \"left\": 286,\n",
      "            \"top\": 177\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99999833\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\",\n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': test_url})), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifiers\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more about what is possible via the Python API, see https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/?python.\n",
    "\n",
    "In essence, the Python API is just wrapping HTTP REST API calls with the `requests` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to classify a local image?\n",
    "\n",
    "There are many labeled datasets used in image recognition research. One of them is the Caltech 101 dataset, see http://www.vision.caltech.edu/Image_Datasets/Caltech101/. The actual dataset is here: http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
    "\n",
    "\n",
    "After downloading and and uncompressing the dataset, we can send the image of a butterfly to Watson.\n",
    "\n",
    "![](images/image_0027.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"monarch butterfly\",\n",
      "              \"score\": 0.87,\n",
      "              \"type_hierarchy\": \"/animal/invertebrate/insect/butterfly/monarch butterfly\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"butterfly\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"insect\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"invertebrate\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"animal\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"viceroy butterfly\",\n",
      "              \"score\": 0.782,\n",
      "              \"type_hierarchy\": \"/animal/invertebrate/insect/butterfly/viceroy butterfly\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"reddish orange color\",\n",
      "              \"score\": 0.889\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"Indian red color\",\n",
      "              \"score\": 0.78\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./images/image_0027.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./images/image_0027.jpg', 'rb') as image_file:\n",
    "    results = visual_recognition.classify(\n",
    "            images_file=image_file,\n",
    "            threshold='0.1')\n",
    "    print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "![](http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.825\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/people\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"Indian red color\",\n",
      "              \"score\": 0.97\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg\",\n",
      "      \"resolved_url\": \"http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mister_t_url = 'http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg'\n",
    "print(json.dumps(visual_recognition.classify(url=mister_t_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cphbusiness.dk/media/75910/lam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"walrus mustache\",\n",
      "              \"score\": 0.559,\n",
      "              \"type_hierarchy\": \"/person/walrus mustache\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.811\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"beard\",\n",
      "              \"score\": 0.538,\n",
      "              \"type_hierarchy\": \"/person/beard\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"mustachio\",\n",
      "              \"score\": 0.511,\n",
      "              \"type_hierarchy\": \"/person/mustachio\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person portrait photo\",\n",
      "              \"score\": 0.51,\n",
      "              \"type_hierarchy\": \"/person/person/person portrait photo\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"taxonomist\",\n",
      "              \"score\": 0.504,\n",
      "              \"type_hierarchy\": \"/person/taxonomist\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"adult person\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/adult person\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"reddish brown color\",\n",
      "              \"score\": 0.742\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.cphbusiness.dk/media/75910/lam.png\",\n",
      "      \"resolved_url\": \"https://www.cphbusiness.dk/media/75910/lam.png\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lars_url = 'https://www.cphbusiness.dk/media/75910/lam.png'\n",
    "print(json.dumps(visual_recognition.classify(url=lars_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cphbusiness.dk/media/74691/ltje.jpg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 25,\n",
      "            \"max\": 28,\n",
      "            \"score\": 0.79470074\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 949,\n",
      "            \"width\": 832,\n",
      "            \"left\": 333,\n",
      "            \"top\": 514\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.999997\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.cphbusiness.dk/media/74691/ltje.jpg\",\n",
      "      \"resolved_url\": \"https://www.cphbusiness.dk/media/74691/ltje.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "line_url = 'https://www.cphbusiness.dk/media/74691/ltje.jpg'\n",
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': line_url})), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Many Faces in an Image\n",
    "\n",
    "![](http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"wraparound\",\n",
      "              \"score\": 0.548,\n",
      "              \"type_hierarchy\": \"/garment/wraparound\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"garment\",\n",
      "              \"score\": 0.548\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"family\",\n",
      "              \"score\": 0.525,\n",
      "              \"type_hierarchy\": \"/person/family\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.526\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.563\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"group of people\",\n",
      "              \"score\": 0.5\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"alizarine red color\",\n",
      "              \"score\": 0.846\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"ivory color\",\n",
      "              \"score\": 0.724\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\",\n",
      "      \"resolved_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n",
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 32,\n",
      "            \"max\": 35,\n",
      "            \"score\": 0.8495456\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 49,\n",
      "            \"width\": 45,\n",
      "            \"left\": 173,\n",
      "            \"top\": 91\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9584368\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 51,\n",
      "            \"max\": 55,\n",
      "            \"score\": 0.6513539\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 52,\n",
      "            \"width\": 46,\n",
      "            \"left\": 239,\n",
      "            \"top\": 68\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.99932075\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 23,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.7988043\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 45,\n",
      "            \"width\": 36,\n",
      "            \"left\": 152,\n",
      "            \"top\": 50\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9971029\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 22,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.73620075\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 42,\n",
      "            \"width\": 40,\n",
      "            \"left\": 31,\n",
      "            \"top\": 94\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99993646\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 19,\n",
      "            \"max\": 22,\n",
      "            \"score\": 0.99972486\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 55,\n",
      "            \"width\": 41,\n",
      "            \"left\": 190,\n",
      "            \"top\": 136\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.999127\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 20,\n",
      "            \"max\": 24,\n",
      "            \"score\": 0.6713774\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 46,\n",
      "            \"width\": 38,\n",
      "            \"left\": 52,\n",
      "            \"top\": 144\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.9985127\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 23,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.84344494\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 49,\n",
      "            \"width\": 48,\n",
      "            \"left\": 228,\n",
      "            \"top\": 145\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99955493\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 28,\n",
      "            \"max\": 33,\n",
      "            \"score\": 0.51941246\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 48,\n",
      "            \"width\": 33,\n",
      "            \"left\": 109,\n",
      "            \"top\": 134\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99695385\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 25,\n",
      "            \"max\": 28,\n",
      "            \"score\": 0.9299795\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 46,\n",
      "            \"width\": 44,\n",
      "            \"left\": 45,\n",
      "            \"top\": 47\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.99030745\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\",\n",
      "      \"resolved_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "group_url = 'http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg'\n",
    "\n",
    "print(json.dumps(visual_recognition.classify(url=group_url), indent=2))\n",
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': group_url})), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training your own Classifier\n",
    "\n",
    "This is your task now. Take some images from different categories from the Caltech 101 dataset and train the neural network with them. For example, as in the following:\n",
    "\n",
    "```python\n",
    "with open('/path/to/butterflies.zip', 'rb') as butterflies, \\\n",
    "     open('/path/to/airplanes.zip'), 'rb') as airplanes:\n",
    "    print(json.dumps(visual_recognition.create_classifier('ButterfliesvsPlanes', \n",
    "                                                          butterflies_positive_examples=butterflies, \n",
    "                                                          negative_examples=airplanes), \n",
    "                     indent=2))\n",
    "```\n",
    "\n",
    "When you created some classifiers, you can list them as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifiers\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does this work? Introduction to Neural Networks.\n",
    "\n",
    "\n",
    "A next step from a single perceptron -as seen in the last lecture- to an image classifier as above is a Multi-layer Perceptron.\n",
    "\n",
    "![](http://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
    "\n",
    "\n",
    "\n",
    "The code in the following is adapted from Chapter 18 \"Neural Networks\" in the Data Science from Scratch book. The code can be found at: https://github.com/joelgrus/data-science-from-scratch/blob/master/code-python3/neural_networks.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 37/20000 [00:00<00:54, 364.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:18<00:00, 144.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(0)   # to get repeatable results\n",
    "input_size = 25  # each input is a vector of length 25\n",
    "\n",
    "num_hidden = 5   # we'll have 5 neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for _ in range(input_size + 1)]\n",
    "                 for _ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for _ in range(num_hidden + 1)]\n",
    "                 for _ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "# 10,000 iterations seems enough to converge\n",
    "for _ in tqdm(range(20000)):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainings Dataset\n",
    "\n",
    "We want to create a Multi-layer Perceptron, which can classifiy -or recognize- the digits from zero to nine for us. In `raw_digits` we create digits consisting out of 5x5 binary pixels. Consequently, each input in our trainings dataset is a binary vector of length 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_digits = [\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           1...1\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"..1..\n",
    "           ..1..\n",
    "           ..1..\n",
    "           ..1..\n",
    "           ..1..\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           11111\n",
    "           1....\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\",\n",
    "        \"\"\"1...1\n",
    "           1...1\n",
    "           11111\n",
    "           ....1\n",
    "           ....1\"\"\",\n",
    "        \"\"\"11111\n",
    "           1....\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           1....\n",
    "           11111\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           ....1\n",
    "           ....1\n",
    "           ....1\"\"\",\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           11111\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\"]\n",
    "\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]\n",
    "\n",
    "\n",
    "inputs = [make_digit(raw_digit) for raw_digit in raw_digits]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For convience and reusabilty, we save the vectors containing the raw digits to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1]\n",
      " [1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(np.array(inputs, dtype=np.int8))\n",
    "#np.savetxt?\n",
    "np.savetxt('./simple_digit_trainingset.csv', np.array(inputs, dtype=np.int8), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,1,1,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1\n",
      "0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0\n",
      "1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1\n",
      "1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1\n",
      "1,0,0,0,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1\n",
      "1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1\n",
      "1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1\n",
      "1,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1\n",
      "1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1\n",
      "1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat simple_digit_trainingset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create to helper functions, one for reading our trainings dataset from a file and a second one, which will plot it for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import csv\n",
    "import webget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "filename = './simple_digit_trainingset.csv'\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            label = reader.line_num - 1\n",
    "            image = np.array(row[:], dtype=np.int8)\n",
    "            data.append((label, image))\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_plot(data):\n",
    "    count = 0\n",
    "    f = plt.figure(figsize=(10, 5))\n",
    "    for idx, row in enumerate(data):\n",
    "        imarray = row[1].reshape((5, 5))\n",
    "        plt.subplot(2, 5, idx + 1)\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        count += 1\n",
    "        plt.title('Label = {}'.format(row[0]))\n",
    "        plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def predict(in_put):\n",
    "    return feed_forward(network, in_put)[-1]\n",
    "\n",
    "\n",
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # print(i,j)\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    #print('----')\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, in_put in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * in_put"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Test-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_testset(data):\n",
    "    count = 0\n",
    "    f = plt.figure(figsize=(10, 5))\n",
    "    data = np.array(data)\n",
    "    for idx, row in enumerate(data):\n",
    "        imarray = row.reshape((5, 5))\n",
    "        plt.subplot(2, len(data), idx + 1)\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        count += 1\n",
    "        plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    return plt\n",
    "\n",
    "\n",
    "test_set = [[0,1,1,1,0,\n",
    "             0,0,0,1,1,\n",
    "             0,0,1,1,0,\n",
    "             0,0,0,1,1,\n",
    "             0,1,1,1,0],\n",
    "            [0,1,1,1,0,\n",
    "             1,0,0,1,1,\n",
    "             0,1,1,1,0,\n",
    "             1,0,0,1,1,\n",
    "             0,1,1,1,0],\n",
    "            [0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0],\n",
    "            [0,1,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0]]\n",
    "print(test_set)\n",
    "plt.show(plot_testset(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [0.93 0.87 0.96 0.97 0.93 0.93 0.94 0.9  0.97 0.86]\n",
      "8 [0.93 0.87 0.96 0.97 0.93 0.93 0.94 0.9  0.97 0.86]\n",
      "8 [0.92 0.87 0.96 0.97 0.93 0.93 0.93 0.89 0.97 0.86]\n",
      "8 [0.92 0.87 0.96 0.97 0.93 0.93 0.94 0.89 0.97 0.86]\n"
     ]
    }
   ],
   "source": [
    "for test_data in test_set:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "3 [0.   0.   0.   0.93 0.   0.   0.   0.01 0.   0.1 ]\n",
    "9 [0.   0.   0.   0.   0.   0.54 0.   0.   0.91 1.  ]\n",
    "1 [0.   0.96 0.03 0.02 0.   0.   0.   0.   0.   0.  ]\n",
    "3 [0.   0.22 0.   0.73 0.   0.   0.   0.   0.   0.  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for test_data in inputs:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks Done Properly...\n",
    "\n",
    "And we still have a bit to go from classifying digits with a Multi-layer Perceptron to a Convolutional Neural Network (CNN), which is the technique that IBM applies in Watson for visual recognition. A modern framework for implementing various types of neural networks is Google's Tensorflow.\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "You can get more information about it here:\n",
    "\n",
    "  * https://www.youtube.com/watch?v=qyvlt7kiQoI\n",
    "  * https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0\n",
    "  * https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py\n",
    "  * https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise!!!\n",
    "\n",
    "Your task is to extend the above example to work with the 'classical' MNIST dataset, which contains many thousands of handwritten digits. Your task is to watch the video https://hooktube.com/watch?v=wQ8BIBpya2k on HookTube (a lighter version of YouTube), which gives an introduction to Google's Tensorflow - a Python framework helping to build neural networks- and you follow the tutorial on https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist.\n",
    "\n",
    "You have to reproduce their solution and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Possible Projects:\n",
    "\n",
    "  * Implementation of a Salient Region Detector: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.228.5552&rep=rep1&type=pdf\n",
    "  * Audio Fingerprinting: http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/\n",
    "  * Survival on the Titanic: https://www.kaggle.com/c/titanic\n",
    "  * Predict forest cover: https://www.kaggle.com/c/forest-cover-type-prediction\n",
    "  * How to get free pizza: https://www.kaggle.com/c/random-acts-of-pizza"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
